# Use a Python base image
FROM python:3.12.1-bullseye AS python-base

# Set the working directory in the container
WORKDIR /app

# Copy your PySpark script into the container
COPY . /app/

# Use a separate OpenJDK image
FROM openjdk:17-slim AS openjdk-base

# Set environment variables
ENV SPARK_VERSION=3.5.0
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/spark
ENV JAVA_HOME=/usr/local/openjdk-17
ENV PATH="${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${JAVA_HOME}/bin:${PATH}"

# Create the /spark directory
RUN mkdir /spark

# update, upgrade, and install wget
RUN apt-get update && apt-get -y upgrade && \
    apt-get install -y --no-install-recommends \
    wget \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Download and extract Spark
RUN wget -qO- "https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" | tar -xzf - -C /spark --strip-components=1

# Use a final image for the combined result
FROM python-base

# Set environment variables for Spark
ENV SPARK_HOME=/spark
ENV PATH="${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"

# Set environment variables for Python
ENV PYSPARK_PYTHON=/usr/local/bin/python

# Copy from the openjdk-base image
COPY --from=openjdk-base /spark /spark
COPY --from=openjdk-base /usr/local/openjdk-17 /usr/local/openjdk-17

# Explicitly set JAVA_HOME
ENV JAVA_HOME /usr/local/openjdk-17

# Print JAVA_HOME for debugging
RUN echo "JAVA_HOME: $JAVA_HOME"

RUN pip install --upgrade pip && pip install --no-cache-dir -r requirements.txt

# CMD [ "python", "./main.py" ]
ENTRYPOINT ["tail", "-f", "/dev/null"]
