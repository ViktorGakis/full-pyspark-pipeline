{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "from pathlib import Path\n",
    "from tests import test_mysql_conx, test_pyspark_con\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MYSQL_CONNECTOR_PATH = \"usr/share/java/mysql-connector-java-8.2.0.jar\"\n",
    "MYSQL_CONNECTOR_FILENAME = \"mysql-connector-j-8.2.0.jar\"\n",
    "MYSQL_CONNECTOR_PATH = f\"./app/mysql_connector/{MYSQL_CONNECTOR_FILENAME}\"\n",
    "Path(MYSQL_CONNECTOR_PATH).exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the connector driver in the proper place in order to be recognizable by pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.add_jars(MYSQL_CONNECTOR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD: Path = Path(\"./app/\")\n",
    "EXAMPLE_INPUT_PATH: Path = CWD / Path(\"./coding_challenge_files/example_input.txt\")\n",
    "TABLE_NAME = \"instruments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining mysql credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'driver': 'com.mysql.cj.jdbc.Driver',\n",
       " 'url': 'jdbc:mysql://db:3306/mydb',\n",
       " 'user': 'root',\n",
       " 'password': 'example'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# database connection info\n",
    "DB_CON_DICT = dict(\n",
    "    user=getenv(\"MYSQL_ROOT_USER\"),\n",
    "    password=getenv(\"MYSQL_ROOT_PASSWORD\"),\n",
    "    host=getenv(\"HOST\"),\n",
    "    port=int(getenv(\"MYSQL_DOCKER_PORT\")),\n",
    "    database=getenv(\"MYSQL_DATABASE\"),\n",
    ")\n",
    "\n",
    "# Configure MySQL connection properties\n",
    "MYSQL_PROPERTIES = {\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"url\": \"jdbc:mysql://{host}:{port}/{database}\".format(**DB_CON_DICT),\n",
    "    \"user\": DB_CON_DICT[\"user\"],\n",
    "    \"password\": DB_CON_DICT[\"password\"],\n",
    "}\n",
    "MYSQL_PROPERTIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Database connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we test the connection to our mysql database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Success\n"
     ]
    }
   ],
   "source": [
    "# test database connection\n",
    "test_mysql_conx(**DB_CON_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Pyspark installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we test if pyspark is properly installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/10 16:40:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|Hello|\n",
      "|World|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pyspark_con()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK PART ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read time series from the file provided and pass all of them to the \"calculation module\".\n",
    "\n",
    "Calculation engine needs to calculate:\n",
    "\n",
    "1. For INSTRUMENT1 – mean\n",
    "\n",
    "1. For INSTRUMENT2 – mean for November 2014\n",
    "\n",
    "1. For INSTRUMENT3 – any other statistical calculation that we can compute \"on-the-fly\" as we read the file (it's up to you)\n",
    "\n",
    "1. For any other instrument from the input file - sum of the newest 10 elements (in terms of the date)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read time series from the file provided and pass all of them to the \"calculation module\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we read and transform the data to a pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INSTRUMENT_NAME: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- VALUE: double (nullable = true)\n",
      "\n",
      "+---------------+-----------+------+\n",
      "|INSTRUMENT_NAME|       DATE| VALUE|\n",
      "+---------------+-----------+------+\n",
      "|    INSTRUMENT1|01-Jan-1996|2.4655|\n",
      "|    INSTRUMENT1|02-Jan-1996|2.4685|\n",
      "|    INSTRUMENT1|03-Jan-1996| 2.473|\n",
      "|    INSTRUMENT1|04-Jan-1996|2.4845|\n",
      "|    INSTRUMENT1|05-Jan-1996|2.4868|\n",
      "+---------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/10 17:21:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Calc_Engine\")\n",
    "    .config(\"spark.jars\", MYSQL_CONNECTOR_PATH)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Specify the path to the .txt file\n",
    "txt_file_path: str = f\"{EXAMPLE_INPUT_PATH}\"\n",
    "\n",
    "# Define the schema with StringType for DATE initially\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(name=\"INSTRUMENT_NAME\", dataType=StringType(), nullable=True),\n",
    "        StructField(name=\"DATE\", dataType=StringType(), nullable=True),\n",
    "        StructField(name=\"VALUE\", dataType=DoubleType(), nullable=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read the .txt file into a PySpark DataFrame\n",
    "extr = spark.read.option(\"delimiter\", \",\").csv(\n",
    "    txt_file_path, header=False, schema=schema\n",
    ")\n",
    "\n",
    "# transform to dataframe\n",
    "df = extr.toDF(\"INSTRUMENT_NAME\", \"DATE\", \"VALUE\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we properly handle the date column as a date datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INSTRUMENT_NAME: string (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- VALUE: double (nullable = true)\n",
      "\n",
      "+---------------+----------+------+\n",
      "|INSTRUMENT_NAME|      DATE| VALUE|\n",
      "+---------------+----------+------+\n",
      "|    INSTRUMENT1|1996-01-01|2.4655|\n",
      "|    INSTRUMENT1|1996-01-02|2.4685|\n",
      "|    INSTRUMENT1|1996-01-03| 2.473|\n",
      "|    INSTRUMENT1|1996-01-04|2.4845|\n",
      "|    INSTRUMENT1|1996-01-05|2.4868|\n",
      "+---------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the DATE column to a DateType using to_date function and an appropriate date format\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "date_format_str = \"dd-MMM-yyyy\"\n",
    "\n",
    "# Convert the string to a DateType using to_date function\n",
    "col_date_str = \"DATE\"\n",
    "col_transformed_to_date = \"DATE\"  # \"transformed_date\"\n",
    "col_formatted_Date = \"DATE\"  # \"formatted_date\"\n",
    "\n",
    "df = df.withColumn(col_transformed_to_date, to_date(df[col_date_str], date_format_str))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the Data by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+\n",
      "|INSTRUMENT_NAME|      DATE|      VALUE|\n",
      "+---------------+----------+-----------+\n",
      "|    INSTRUMENT1|2014-12-19|   3.475244|\n",
      "|    INSTRUMENT2|2014-12-19|9.226391955|\n",
      "|    INSTRUMENT3|2014-12-19|     119.37|\n",
      "|    INSTRUMENT1|2014-12-18|   3.460937|\n",
      "|    INSTRUMENT2|2014-12-18|9.223690651|\n",
      "|    INSTRUMENT3|2014-12-18|    119.275|\n",
      "|    INSTRUMENT1|2014-12-17|   3.404217|\n",
      "|    INSTRUMENT2|2014-12-17|9.222419168|\n",
      "|    INSTRUMENT3|2014-12-17|   117.2525|\n",
      "|    INSTRUMENT1|2014-12-16|   3.371051|\n",
      "+---------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.orderBy(col(\"DATE\").desc())\n",
    "\n",
    "df.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
